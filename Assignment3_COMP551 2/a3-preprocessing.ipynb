{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import collections \n",
    "import re \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "IMDB_train = pd.read_csv('hwk3_datasets/IMDB-train.txt', sep='\\t', header=None)\n",
    "yelp_train = pd.read_csv('hwk3_datasets/yelp-train.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find(s, el): \n",
    "    for i in s.index: \n",
    "        if s[i]==el: \n",
    "            return i\n",
    "    return None\n",
    "\n",
    "IMDB_train[0]=(IMDB_train[0].str.replace('[^\\w\\s]', '')).str.replace('\\d+','').str.lower()\n",
    "#yelp_train[0]=(yelp_train[0].str.replace('[^\\w\\s]', '')).str.replace('\\d+','').str.lower()\n",
    "#freq[i] gets you the frequencies of the words \n",
    "freq = IMDB_train[0].str.split(expand=True).stack().value_counts()[:10000]\n",
    "# every column in freq to .stack() is an example\n",
    "# freq .valuecounts()[:10000]\n",
    "\n",
    "\n",
    "for i in range(len(IMDB_train[0])):\n",
    "    IMDB2=IMDB_train.iloc[i,0].split(\" \")\n",
    "    for j in range(len(IMDB2)):\n",
    "        try: \n",
    "            IMDB2[j] = vocab.loc(IMDB2[j])\n",
    "        except ValueError: \n",
    "            IMDB2[j] = -1\n",
    "\n",
    "    #df = pd.DataFrame(np.array(IMDB_train))\n",
    "    #print(df)\n",
    "#print(type(IMDB_train))\n",
    "#print(IMDB_train)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripdata(data): \n",
    "    data=(data.str.replace('[^\\w\\s]', '')).str.replace('\\d+','').str.lower()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwords(data):\n",
    "    #vocab1 contains all the words\n",
    "    vocab1 = data.str.split(expand=True).stack()\n",
    "    #printed vocab of top 10000 words \n",
    "    words=(vocab1.value_counts()[:10000]).index.tolist()\n",
    "    freq = (vocab1.value_counts()[:10000]).tolist()\n",
    "    return words, freq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train[0] = stripdata(IMDB_train[0])\n",
    "words,freq = getwords(IMDB_train[0])\n",
    "index = np.arange(0,10000)\n",
    "\n",
    "IMDB_vocab = list(np.column_stack((np.array(words),np.array(index),np.array(freq))))\n",
    "pd.DataFrame(IMDB_vocab).to_csv('IMDB-vocab.txt',sep='\\t',header=None,index=False)\n",
    "\n",
    "yelp_train[0] = stripdata(yelp_train[0])\n",
    "words1,freq1 = np.array(getwords(yelp_train[0]))\n",
    "index = np.arange(0,10000)\n",
    "\n",
    "yelp_vocab = list(np.column_stack((np.array(words1),np.array(index),np.array(freq1))))\n",
    "pd.DataFrame(yelp_vocab).to_csv('yelp-vocab.txt',sep='\\t',header=None,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, words): \n",
    "    words=list(words)\n",
    "    for i in range(len(data[0])): \n",
    "        review = data.iloc[i,0].split(\" \")\n",
    "        for j in range(0, len(review)): \n",
    "            if review[j] in words: \n",
    "                review[j] = words.index(review[j])\n",
    "            else: \n",
    "                review[j] = ' '\n",
    "        data.iloc[i,0] = \" \".join(str(e) for e in review)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing train\n",
    "IMDB_train[0] = stripdata(IMDB_train[0])\n",
    "IMDB_train_prep = preprocess(IMDB_train, words)\n",
    "IMDB_train_prep.to_csv('IMDB-train1.txt',sep='\\t',header=None,index=False)\n",
    "yelp_train[0] = stripdata(yelp_train[0])\n",
    "yelp_train_prep = preprocess(yelp_train, words1)\n",
    "yelp_train_prep.to_csv('yelp-train1.txt',sep='\\t',header=None,index=False)\n",
    "\n",
    "\n",
    "IMDB_valid = pd.read_csv('hwk3_datasets/IMDB-valid.txt', sep='\\t', header=None)\n",
    "yelp_valid = pd.read_csv('hwk3_datasets/yelp-valid.txt', sep='\\t', header=None)\n",
    "IMDB_test = pd.read_csv('hwk3_datasets/IMDB-test.txt', sep='\\t', header=None)\n",
    "yelp_test = pd.read_csv('hwk3_datasets/yelp-test.txt', sep='\\t', header=None)\n",
    "\n",
    "#processing valid \n",
    "IMDB_valid[0] = stripdata(IMDB_valid[0])\n",
    "IMDB_valid_prep = preprocess(IMDB_valid, words)\n",
    "IMDB_valid_prep.to_csv('IMDB-valid.txt',sep='\\t',header=None,index=False)\n",
    "yelp_valid[0] = stripdata(yelp_valid[0])\n",
    "yelp_valid_prep = preprocess(yelp_valid, words1)\n",
    "yelp_valid_prep.to_csv('yelp-valid.txt',sep='\\t',header=None,index=False)\n",
    "\n",
    "#processing test\n",
    "IMDB_test[0] = stripdata(IMDB_test[0])\n",
    "IMDB_test_prep = preprocess(IMDB_test, words)\n",
    "IMDB_test_prep.to_csv('IMDB-test.txt',sep='\\t',header=None,index=False)\n",
    "yelp_test[0] = stripdata(yelp_test[0])\n",
    "yelp_test_prep = preprocess(yelp_test, words1)\n",
    "yelp_test_prep.to_csv('yelp-test.txt',sep='\\t',header=None,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwords(data): \n",
    "    vocab1 = data.str.split(expand=True).stack().value_counts()\n",
    "    return vocab1\n",
    "\n",
    "def preprocessing_bow(data, words): \n",
    "    data = stripdata(data)\n",
    "    binary = np.zeros(len(data),len(words))\n",
    "    for i in range(len(data[0])):\n",
    "        review = data.iloc[i,0].split(\" \")\n",
    "        for j in range(0, len(review)): \n",
    "            if review[j] in words:\n",
    "                index = words.index(review[j])\n",
    "                binary[index] = 1\n",
    "    return binary\n",
    "\n",
    "def preprocessing_fbow(data, words, freq):\n",
    "    total_freq = sum(freq, axis=0)\n",
    "    data = stripdata(data)\n",
    "    frequency = np.zeros(len(data), len(words))\n",
    "    for i in range(len(data[0])):\n",
    "        review = data.iloc[i,0].split(\" \")\n",
    "        vocab1 = getwords(review)\n",
    "        for j in range(0, len(review)): \n",
    "            if review[j] in words: \n",
    "                freq = vocab1[review[j]]\n",
    "                index = words.index(review[j])\n",
    "                frequency[index]=(freq/total_freq)          \n",
    "    return frequency\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
